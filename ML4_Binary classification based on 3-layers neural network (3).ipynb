{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification based on 3 layers neural network (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First layer\n",
    "\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]}$ : $X$ denotes the input data\n",
    "\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$ : $g^{[1]}$ is the activation function at the first layer\n",
    "\n",
    "#### Second layer\n",
    "\n",
    "$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "\n",
    "$A^{[2]} = g^{[2]}(Z^{[2]})$ : $g^{[2]}$ is the activation function at the second layer\n",
    "\n",
    "#### Third layer\n",
    "\n",
    "$Z^{[3]} = W^{[3]} A^{[2]} + b^{[3]}$\n",
    "\n",
    "$A^{[3]} = g^{[3]}(Z^{[3]})$ : $g^{[3]}$ is the activation function at the third (output) layer\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "- Sigmoid\n",
    "\n",
    "    $g(z) = \\frac{1}{1 + \\exp^{-z}}$\n",
    "\n",
    "- tanh\n",
    "\n",
    "    $g(z) = \\frac{\\exp^{z} - \\exp^{-z}}{\\exp^{z} + \\exp^{-z}}$\n",
    "\n",
    "- ReLU\n",
    "\n",
    "    $g(z) = \\max(0, z)$\n",
    "\n",
    "- Leaky ReLU\n",
    "\n",
    "    $g(z) = \\max(\\alpha z, z), \\quad \\alpha \\in \\mathbb{R}^+$\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "- The sizes of the hidden layers and the output layer should be determined with respect to the validation accuracy obtained by the network architecture with all the activation functions being sigmoid functions. ($g^{[1]} = g^{[2]} = g^{[3]} =$ Sigmoid)\n",
    "- Apply different activation functions at all the layers except the output layer that should be Sigmoid function\n",
    "- Apply different activation functions at different layers except the output layer that should be Sigmoid function\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- The dataset consists of human images and horse images for the training and the validation\n",
    "- The classifier should be trained using the training set\n",
    "- The classifier should be tested using the validation set\n",
    "- Vectorize an input image matrix into a column vector\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- Write codes in python programming\n",
    "- Use jupyter notebook for the programming environment\n",
    "- You can use any libarary\n",
    "- You have to write your own functions for the followings:\n",
    "    - compute the forward propagation\n",
    "    - compute the backward propagation\n",
    "    - compute the loss\n",
    "    - compute the accuracy\n",
    "    - compute the gradient of the model parameters with respect to the loss\n",
    "    - update the model parameters\n",
    "    - plot the results\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- Apply the gradient descent algorithm with an appropriate learning rate\n",
    "- Apply the number of iterations that lead to the convergence of the algorith\n",
    "- Use the vectorization scheme in the computation of gradients and the update of the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ Implementation ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Libraries and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Global Variables\n",
    "train_data_path = './horse-or-human/train'\n",
    "validation_data_path = './horse-or-human/validation'\n",
    "\n",
    "layer_dims = [10000,50,10,1]    # number of units(Neurons) in each layer\n",
    "learning_rate = 0.02            # step size per each epoch (iteration)\n",
    "threshold = 0.1                 # minimum of cost\n",
    "max_epoch = 2500                # maximum number of epoch (iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Generate Input matrix X and Output vector Y from training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_inputs(image_path) :\n",
    "    transform = transforms.Compose([transforms.Grayscale(),transforms.ToTensor(),])\n",
    "    # the code transforms.Grayscale() is for changing the size [3,100,100] to [1, 100, 100]\n",
    "    # (notice : [channel, height, width] )\n",
    "    image_set = torchvision.datasets.ImageFolder(root=image_path, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(image_set, batch_size=1, shuffle=False, num_workers=1)  \n",
    "\n",
    "    for i,data in enumerate(loader) :\n",
    "        image, label = data\n",
    "        image = image.view(10000,1)\n",
    "        label = label.view(1,1).type(torch.FloatTensor)\n",
    "        \n",
    "        if i == 0 :\n",
    "            t_images = image\n",
    "            t_labels = label\n",
    "        else :\n",
    "            t_images = torch.cat((t_images,image),dim = 1)\n",
    "            t_labels = torch.cat((t_labels,label),dim = 1)\n",
    "        \n",
    "        images = t_images.numpy()\n",
    "        labels = t_labels.numpy()\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def Sigmoid_backward(dA, Z):\n",
    "    t_A = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * t_A * (1-t_A)\n",
    "    return dZ\n",
    "\n",
    "def ReLU(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n",
    "\n",
    "def ReLU_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0    \n",
    "    return dZ\n",
    "\n",
    "def Leaky_ReLU(Z) :\n",
    "    A = np.maximum(0.01*Z, Z)\n",
    "    return A\n",
    "\n",
    "def Leaky_ReLU_backward(dA,Z) :\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z < 0] *= 0.01\n",
    "    return dZ\n",
    "    \n",
    "def Tanh(Z) :\n",
    "    A = np.tanh(Z)\n",
    "    return A\n",
    "\n",
    "def Tanh_backward(dA, Z) :\n",
    "    t_A = np.tanh(Z)\n",
    "    dZ = dA * (1 - np.square(t_A))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Cost and Parameter initialization / update Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_parameters(n) :\n",
    "    parameters = dict()\n",
    "    \n",
    "    # Initalize W[i] and b[i] for i in [1,L-1]\n",
    "    for l in range(1,len(n)) :\n",
    "        parameters['W'+str(l)] = np.random.randn(n[l],n[l-1]) / np.sqrt(n[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros((n[l],1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def update_parameters(parameters,gradients,learning_rate) :\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Update W[i] and b[i] for i in [1,L]\n",
    "    for l in range(1,L+1) :\n",
    "        dW, db = gradients['dW'+str(l)], gradients['db'+str(l)]\n",
    "        parameters['W'+str(l)] -= learning_rate * dW\n",
    "        parameters['b'+str(l)] -= learning_rate * db\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def cost_computation(AL, Y) :\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-np.dot(Y,np.log(AL).T) - np.dot(1-Y,np.log(1-AL).T)) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_Z_computation(A_prev,W,b) :\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    return Z\n",
    "\n",
    "def forward_A_computation(A_prev,W,b,activation) :\n",
    "    assert activation in ['sigmoid','relu','tanh','leaky_relu']\n",
    "    \n",
    "    Z = forward_Z_computation(A_prev,W,b)\n",
    "    if activation == 'sigmoid' :\n",
    "        A = Sigmoid(Z)\n",
    "    elif activation == 'relu' :\n",
    "        A = ReLU(Z)\n",
    "    elif activation == 'tanh' :\n",
    "        A = Tanh(Z)\n",
    "    else :\n",
    "        A = Leaky_ReLU(Z)\n",
    "    \n",
    "    cache = ((A_prev,W,b),Z)\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation(X,parameters,activations) :\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1,L+1) :\n",
    "        A_prev, W, b = A, parameters['W'+str(l)], parameters['b'+str(l)]\n",
    "        A,cache = forward_A_computation(A_prev,W,b,activations[l-1])\n",
    "        caches.append(cache)    \n",
    "    AL = A\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_params_dev_computation(dZ, cache) :\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,a_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_params_dev_computation(dA,cache,activation) :\n",
    "    assert activation in ['sigmoid','relu','tanh','leaky_relu']\n",
    "    (A_prev,W,b),Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    if activation == 'sigmoid' :\n",
    "        dZ = Sigmoid_backward(dA,Z)\n",
    "    elif activation == 'relu' :\n",
    "        dZ = ReLU_backward(dA,Z)\n",
    "    elif activation == 'tanh' :\n",
    "        dZ = Tanh_backward(dA,Z)\n",
    "    else :\n",
    "        dZ = Leaky_ReLU_backward(dA,Z)\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_propagation(AL, Y, caches,activations) :\n",
    "    gradients = dict()\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "    cache = caches[L-1]\n",
    "    dA_prev, dW, db = backward_params_dev_computation(dAL,cache,activations[L-1])\n",
    "    gradients['dA'+str(L)] = dA_prev\n",
    "    gradients['dW'+str(L)] = dW\n",
    "    gradients['db'+str(L)] = db\n",
    "    \n",
    "    for l in reversed(range(1,L)) :\n",
    "        dA = gradients['dA' + str(l+1)]\n",
    "        dA_prev, dW, db = backward_params_dev_computation(dA,caches[l-1],activations[l-1])\n",
    "        gradients['dA'+str(l)] = dA_prev\n",
    "        gradients['dW'+str(l)] = dW\n",
    "        gradients['db'+str(l)] = db\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(AL,Y) :\n",
    "    correct = np.zeros(Y.shape)\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    prediction = AL > 0.5\n",
    "    correct = prediction == Y\n",
    "    \n",
    "    accuracy = np.sum(correct) / m\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_3_layers(X,Y,t_X,t_Y,n,activations,learning_rate,threshold,max_epoch) :\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs, t_costs = [],[]\n",
    "    accuracy, t_accuracy = [],[]\n",
    "    parameters = initalize_parameters(n)\n",
    "    \n",
    "    for epoch in range(max_epoch) :\n",
    "        AL, caches = forward_propagation(X, parameters, activations)\n",
    "        t_AL, _t = forward_propagation(t_X, parameters, activations)\n",
    "        \n",
    "        cost = cost_computation(AL, Y)\n",
    "        t_cost = cost_computation(t_AL, t_Y)\n",
    "        \n",
    "        gradients = backward_propagation(AL, Y, caches,activations)\n",
    "        \n",
    "        parameters = update_parameters(parameters,gradients,learning_rate)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        t_costs.append(t_cost)\n",
    "        \n",
    "        accuracy.append(predict(AL,Y))\n",
    "        t_accuracy.append(predict(t_AL,t_Y))\n",
    "        \n",
    "#        if  epoch % 10 == 0:\n",
    "#            print (\"Cost after iteration %i: %f\" %(epoch, cost))\n",
    "        if cost < threshold :\n",
    "            break\n",
    "            \n",
    "    return costs, accuracy, t_costs, t_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) Plot Loss and Accuracy Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResult(costs,accuracy,title) :\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    plt.subplot(121)\n",
    "    plt.title('Costs', fontsize = 15, color = 'black')\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.subplot(122)\n",
    "    plt.title('Accuracy', fontsize = 15, color = 'black')\n",
    "    plt.plot(np.squeeze(accuracy))\n",
    "    plt.suptitle(title, fontsize = 20, color = 'black',position=(0.5, 1.0+0.05))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "\n",
    "\n",
    "- Plot the training loss at every iteration (x-axis: iteration, y-axis: loss)\n",
    "- Plot the validation loss at every iteration (x-axis: iteration, y-axis: loss)\n",
    "- Plot the training accuracy at every iteration (x-axis: iteration, y-axis: accuracy)\n",
    "- Plot the validation accuracy at every iteration (x-axis: iteration, y-axis: accuracy)\n",
    "- Present the table for the final accuracy and loss with training and validation datasets with your best neural network architecture as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = initialize_inputs(train_data_path)\n",
    "t_X, t_Y = initialize_inputs(validation_data_path)\n",
    "\n",
    "n = [10000,50,10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  $g^{[1]}, g^{[2]}$ are ReLU and $g^{[3]}$ is Sigmoid\n",
    "\n",
    "- Learning curves\n",
    "- Loss and Accuracy table \n",
    "\n",
    "| dataset    | loss       | accuracy   | \n",
    "|:----------:|:----------:|:----------:|\n",
    "| training   |  0.18629   |   95.33%   |\n",
    "| validation |  0.40157   |   80.86%   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu','relu','sigmoid']\n",
    "learning_rate = 0.002\n",
    "thres_hold = 0.01\n",
    "max_epoch = 3000\n",
    "\n",
    "costs, accuracy, t_costs, t_accuracy = NN_3_layers(X,Y,t_X,t_Y,n,activations,learning_rate,threshold,max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotResult(costs, accuracy,'Training')\n",
    "plotResult(t_costs, t_accuracy, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss       : %.5f\" % costs[-1])\n",
    "print(\"Training Accuracy   : %.2f %%\" % (accuracy[-1] * 100))\n",
    "print(\"Validation Loss     : %.5f\" % t_costs[-1])\n",
    "print(\"Validation Accuracy : %.2f %%\" % (t_accuracy[-1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = t_accuracy.index(max(t_accuracy))\n",
    "max_validation_cost, max_validation_accuracy = t_costs[i], t_accuracy[i]\n",
    "\n",
    "print(\"Max Validation Loss     : %.5f\" % max_validation_cost)\n",
    "print(\"Max Validation Accuracy : %.2f %%\" % (max_validation_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
